{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<center>Time Series Forecasting Concepts and Practicals</center>**\n",
    "- Time series forecasting uses historical data to predict future events.\n",
    "- Time series data is collected over time and helps model trends, patterns, and behaviors.\n",
    "- Various approaches to time series forecasting exist, including statistical and machine learning methods.\n",
    "- The autoregressive integrated moving average (ARIMA) model is a common statistical method for time series forecasting.\n",
    "- ARIMA fits a linear equation to past data and uses it to make predictions.\n",
    "- Seasonal decomposition is another statistical method that breaks data into trend, seasonality, and residual components for accurate forecasts.\n",
    "\n",
    "    ![Alt Text](./Images/7_Day_Rolling_Mean.webp) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Series Forecasting vs Regression**\n",
    "- Time series analysis and regression are both statistical techniques used in data analysis and predictive modeling.\n",
    "- Time series analysis is for time-dependent data, like stock prices or sales data, and involves identifying patterns and trends to make future predictions.\n",
    "- Regression models the relationship between dependent and independent variables and is used in various fields to understand relationships and make predictions.\n",
    "- Time series analysis is focused on temporal data, while regression deals with general relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Forecasting**\n",
    "- There are different types of forecasting methods.\n",
    "- Time series forecasting predicts future values based on past values using methods like exponential smoothing and ARIMA.\n",
    "- Causal forecasting predicts future values based on other variables using techniques like linear regression.\n",
    "- Qualitative forecasting relies on expert judgment and includes methods like the Delphi method.\n",
    "- Intuitive forecasting relies on intuition and is used when data or information is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic steps of Time series forecasting**\n",
    "\n",
    "1. **Data Collection and Cleaning:**\n",
    "   - Collect and clean the data by removing missing or invalid data points.\n",
    "   - Transform the data if needed to stabilize variance or achieve stationarity.\n",
    "\n",
    "2. **Exploration of Data:**\n",
    "   - Explore the data using visualization techniques like line charts and histograms.\n",
    "   - Identify trends and seasonality in the data.\n",
    "\n",
    "3. **Selecting a Forecasting Method:**\n",
    "   - Choose an appropriate time series forecasting method based on your data and goals.\n",
    "   - Common methods include exponential smoothing, Holt-Winters, and ARIMA models.\n",
    "\n",
    "4. **Model Fitting:**\n",
    "   - Estimate model parameters to fit the selected forecasting method to your data.\n",
    "\n",
    "5. **Making Predictions:**\n",
    "   - Use the fitted model to make predictions for future time periods.\n",
    "   - Specify a forecast horizon for the desired number of periods ahead.\n",
    "\n",
    "6. **Model Evaluation:**\n",
    "   - Evaluate the model's performance by calculating metrics like MAE and RMSE.\n",
    "   - Assess the accuracy of the generated forecasts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts in Time Series Forecasting**\n",
    "\n",
    "1. **Trend:** Long-term increase or decrease in data, with positive trends indicating data growth over time and negative trends indicating data decline.\n",
    "\n",
    "2. **Seasonality:** Repeating patterns in data at regular intervals (e.g., monthly, quarterly), such as higher winter jacket sales in winter and lower sales in summer.\n",
    "\n",
    "3. **Stationarity:** Data is stationary when its statistical properties (e.g., mean, variance) remain constant over time. Transformations may be needed to achieve stationarity.\n",
    "\n",
    "4. **Autocorrelation:** Degree of correlation between a data point and previous points, helping identify data patterns and select appropriate forecasting methods.\n",
    "\n",
    "5. **Forecast Horizon:** The number of future periods for which predictions are made, e.g., forecasting sales for the next 6 months in monthly sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seasonality vs Cyclicity**\n",
    "- Seasonality involves repeating patterns in data at regular intervals (e.g., monthly, quarterly, annually) due to external factors like weather or holidays.\n",
    "- Cyclicity involves patterns that repeat at irregular intervals due to internal factors like economic cycles.\n",
    "- Seasonality example: Retail store sales increase during the holiday season and decrease in the summer.\n",
    "- Cyclicity example: Unemployment rates show high values during economic recessions and low values during economic expansions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additive and Multiplicative Seasonal Decomposition**\n",
    "- Seasonal decomposition is a statistical method that breaks down time series data into trend, seasonality, and residuals.\n",
    "- There are two main types of seasonal decomposition: additive and multiplicative.\n",
    "- Additive decomposition adds the trend, seasonality, and residuals together, suitable when trend and seasonality magnitudes remain constant.\n",
    "- Multiplicative decomposition multiplies the trend, seasonality, and residuals, suitable when trend and seasonality magnitudes vary.\n",
    "- Additive example: Retail store sales with consistent holiday season spikes and summer slumps.\n",
    "- Multiplicative example: Electricity consumption with higher summer usage and lower winter demand.\n",
    "\n",
    "    ![Alt Text](./Images/Additive_and_Multiplicative_Seaonal_Decomposition.png) \n",
    "    \n",
    "    ![Alt Text](./Images/Decomposition_of_Additive_and_Multiplicative.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-Stationary vs. Stationary**\n",
    "- Stationary time series: Statistical properties (mean, variance, autocovariance) remain constant over time, making them stable and predictable.\n",
    "- Non-stationary time series: Statistical properties change over time, often exhibiting trends or seasonality, making them harder to model and forecast.\n",
    "- To make a non-stationary time series stationary, apply transformations like differencing (removing trends) or logging (stabilizing variance).\n",
    "\n",
    "    ![Alt Text](./Images/Stationary.png) \n",
    "\n",
    "\n",
    "**Time Series Stationarity and First-Order Differencing**\n",
    "\n",
    "- Time series data can be non-stationary, with changing mean and variance over time. To make it stationary, we often use **first-order differencing** by calculating the difference between consecutive values.\n",
    "\n",
    "- For example:\n",
    "\n",
    "    | Time | Value | Difference |\n",
    "    |------|-------|------------|\n",
    "    | 1    | 100   | NA         |\n",
    "    | 2    | 105   | 5          |\n",
    "    | 3    | 110   | 5          |\n",
    "    | 4    | 115   | 5          |\n",
    "\n",
    "- In this table, differencing stabilizes the mean and variance, turning the non-stationary data into a stationary time series. This is useful for building time series models.\n",
    "- Note that not all time series can be made stationary through differencing; some may require advanced techniques like seasonal decomposition models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autocorrelation function (ACF) and Partial Autocorrelation Function (PACF)**\n",
    "- Autocorrelation function (ACF) and Partial Autocorrelation Function (PACF) are statistical measures used to analyze time series data.\n",
    "- ACF measures the correlation between a time series and its lagged values, indicating how past values relate to the present.\n",
    "- PACF measures the correlation between a time series and its lagged values, excluding correlations at lower-order lags.\n",
    "- ACF and PACF are used in time series analysis to identify patterns, trends, and model orders for forecasting.\n",
    "\n",
    "**The ACF plot can provide answers to the following questions:**\n",
    "- Is the observed time series white noise/random?\n",
    "- Is an observation related to an adjacent observation, an observation twice-removed, and so on?\n",
    "- Can the observed time series be modeled with an MA model? If yes, what is the order?\n",
    "\n",
    "**The PACF plot can provide answers to the following question:**\n",
    "- Can the observed time series be modeled with an AR model? If yes, what is the order?\n",
    "\n",
    "    ![Alt Text](./Images/ACF_PACF.webp) \n",
    "\n",
    "- ACF and PACF both start with a lag of 0, resulting in a correlation of 1 with the time series itself.\n",
    "- A blue area in ACF and PACF plots represents a 95% confidence interval, indicating significance.\n",
    "- Values within the blue area are statistically close to zero, while those outside are statistically non-zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting ACF and PACF plots**\n",
    "\n",
    "1. \n",
    "\n",
    "![Alt Text](./Images/Interpretation_ACF_PACF_1.webp) \n",
    "\n",
    "**Observations**\n",
    "- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n",
    "- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 2) observations in PACF plot\n",
    "- Geometric decay in ACF plot\n",
    "\n",
    "2.\n",
    "\n",
    "![Alt Text](./Images/Interpretation_ACF_PACF_2.webp)\n",
    "\n",
    "**Observations**\n",
    "- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n",
    "- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 2) observations in ACF plot\n",
    "- Geometric decay in PACF plot\n",
    "\n",
    "3.\n",
    "\n",
    "![Alt Text](./Images/Interpretation_ACF_PACF_3.webp)\n",
    "\n",
    "**Observations**\n",
    "- There’s only one autocorrelation that is significantly non-zero at a lag of 0. Therefore, the time series is random.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "- https://medium.com/@manuktiwary/time-series-forecasting-concepts-and-methods-with-implementation-examples-edaf40dceee5\n",
    "- https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
